@article{wei2025swerl,
  title={SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution}, 
  author={Yuxiang Wei and Olivier Duchenne and Jade Copet and Quentin Carbonneaux and Lingming Zhang and Daniel Fried and Gabriel Synnaeve and Rishabh Singh and Sida I. Wang},
  year={2025},
  journal={arXiv preprint arXiv:2502.18449},
  url={https://arxiv.org/abs/2502.18449},
  addendum={\textbf{NeurIPS'25}}
}

@inproceedings{
wei2024selfcodealign,
title={SelfCodeAlign: Self-Alignment for Code Generation},
author={Yuxiang Wei and Federico Cassano and Jiawei Liu and Yifeng Ding and Naman Jain and Zachary Mueller and Harm de Vries and Leandro Von Werra and Arjun Guha and Lingming Zhang},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
addendum={\textbf{NeurIPS'24}},
url={https://openreview.net/forum?id=xXRnUU7xTL}
}

@inproceedings{
snowcoder,
title={Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining},
author={Yuxiang Wei and Hojae Han and Rajhans Samdani},
booktitle={ICLR 2025 Third Workshop on Deep Learning for Code},
year={2025},
url={https://openreview.net/forum?id=lP44oj9cWU},
addendum={\textbf{DL4C@ICLR'25}},
}

@inproceedings{
evalperf,
title={Evaluating Language Models for Efficient Code Generation},
author={Jiawei Liu and Songrun Xie and Junhao Wang and Yuxiang Wei and Yifeng Ding and Lingming Zhang},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=IBCBMeAhmC},
addendum={\textbf{COLM'24}}
}

@inproceedings{
liu2024repoqa,
title={Repo{QA}: Evaluating Long Context Code Understanding},
author={Jiawei Liu and Jia Le Tian and Vijay Daita and Yuxiang Wei and Yifeng Ding and Yuhan Katherine Wang and Jun Yang and Lingming Zhang},
booktitle={First Workshop on Long-Context Foundation Models @ ICML 2024},
year={2024},
url={https://openreview.net/forum?id=hK9YSrFuGf},
addendum={\textbf{LCFM@ICML'24}}
}

misc{sc2instruct,
  title={StarCoder2-Instruct: Fully Transparent and Permissive Self-Alignment for Code Generation},
  author={Yuxiang Wei and Federico Cassano and Jiawei Liu and Yifeng Ding and Naman Jain and Harm de Vries and Leandro von Werra and Arjun Guha and Lingming Zhang},
  howpublished = {\url{https://huggingface.co/blog/sc2-instruct}},
  year={2024},
}

@article{starcoder2,
      title={StarCoder 2 and The Stack v2: The Next Generation}, 
      comment={\textbf{The BigCode Team}},
      author={Anton Lozhkov and Raymond Li and Loubna Ben Allal and Federico Cassano and Joel Lamy-Poirier and Nouamane Tazi and Ao Tang and Dmytro Pykhtar and Jiawei Liu and Yuxiang Wei and Tianyang Liu and Max Tian and Denis Kocetkov and Arthur Zucker and Younes Belkada and Zijian Wang and Qian Liu and Dmitry Abulkhanov and Indraneil Paul and Zhuang Li and Wen-Ding Li and Megan Risdal and Jia Li and Jian Zhu and Terry Yue Zhuo and Evgenii Zheltonozhskii and Nii Osae Osae Dade and Wenhao Yu and Lucas Krauß and Naman Jain and Yixuan Su and Xuanli He and Manan Dey and Edoardo Abati and Yekun Chai and Niklas Muennighoff and Xiangru Tang and Muhtasham Oblokulov and Christopher Akiki and Marc Marone and Chenghao Mou and Mayank Mishra and Alex Gu and Binyuan Hui and Tri Dao and Armel Zebaze and Olivier Dehaene and Nicolas Patry and Canwen Xu and Julian McAuley and Han Hu and Torsten Scholak and Sebastien Paquet and Jennifer Robinson and Carolyn Jane Anderson and Nicolas Chapados and Mostofa Patwary and Nima Tajbakhsh and Yacine Jernite and Carlos Muñoz Ferrandis and Lingming Zhang and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro von Werra and Harm de Vries},
      journal={arXiv preprint arXiv:2402.19173},
      year={2024},
}

@inproceedings{xft,
    title = "XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging Upcycled Mixture-of-Experts",
    author = "Ding, Yifeng  and
      Liu, Jiawei  and
      Wei, Yuxiang  and
      Zhang, Lingming",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.699",
    pages = "12941--12955",
    address = "Bangkok, Thailand",
    addendum={\textbf{ACL'24}}
}


@InProceedings{magicoder,
  title = 	 {Magicoder: Empowering Code Generation with {OSS}-Instruct},
  author =       {Wei, Yuxiang and Wang, Zhe and Liu, Jiawei and Ding, Yifeng and Zhang, Lingming},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {52632--52657},
  year = 	 {2024},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/wei24h/wei24h.pdf},
  url = 	 {https://proceedings.mlr.press/v235/wei24h.html},
addendum={\textbf{ICML'24}}
}

@inproceedings{repilot,
author = {Wei, Yuxiang and Xia, Chunqiu Steven and Zhang, Lingming},
title = {Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616271},
doi = {10.1145/3611643.3616271},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {172–184},
numpages = {13},
location  = {San Francisco, CA, USA},
keywords = {Completion Engine, Program Repair, Large Language Model},
addendum = {\textbf{ESEC/FSE'23}}
}

@inproceedings{repairstudy,
author = {Xia, Chunqiu Steven and Wei, Yuxiang and Zhang, Lingming},
title = {Automated Program Repair in the Era of Large Pre-Trained Language Models},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00129},
doi = {10.1109/ICSE48619.2023.00129},
abstract = {Automated Program Repair (APR) aims to help developers automatically patch software bugs. However, current state-of-the-art traditional and learning-based APR techniques face the problem of limited patch variety, failing to fix complicated bugs. This is mainly due to the reliance on bug-fixing datasets to craft fix templates (traditional) or directly predict potential patches (learning-based). Large Pre-Trained Language Models (LLMs), trained using billions of text/code tokens, can potentially help avoid this issue. Very recently, researchers have directly leveraged LLMs for APR without relying on any bug-fixing datasets. Meanwhile, such existing work either failed to include state-of-the-art LLMs or was not evaluated on realistic datasets. Thus, the true power of modern LLMs on the important APR problem is yet to be revealed.In this work, we perform the first extensive study on directly applying LLMs for APR. We select 9 recent state-of-the-art LLMs, including both generative and infilling models, ranging from 125M to 20B in size. We designed 3 different repair settings to evaluate the different ways we can use LLMs to generate patches: 1) generate the entire patch function, 2) fill in a chunk of code given the prefix and suffix 3) output a single line fix. We apply the LLMs under these repair settings on 5 datasets across 3 different languages and compare different LLMs in the number of bugs fixed, generation speed and compilation rate. We also compare the LLMs against recent state-of-the-art APR tools. Our study demonstrates that directly applying state-of-the-art LLMs can already substantially outperform all existing APR techniques on all our datasets. Among the studied LLMs, the scaling effect exists for APR where larger models tend to achieve better performance. Also, we show for the first time that suffix code after the buggy line (adopted in infilling-style APR) is important in not only generating more fixes but more patches with higher compilation rate. Besides patch generation, the LLMs consider correct patches to be more natural than other ones, and can even be leveraged for effective patch ranking or patch correctness checking. Lastly, we show that LLM-based APR can be further substantially boosted via: 1) increasing the sample size, and 2) incorporating fix template information.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1482–1494},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23},
addendum={\textbf{ICSE'23}}
}

@article{tzer,
author = {Liu, Jiawei and Wei, Yuxiang and Yang, Sen and Deng, Yinlin and Zhang, Lingming},
title = {Coverage-Guided Tensor Compiler Fuzzing with Joint IR-Pass Mutation},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3527317},
doi = {10.1145/3527317},
abstract = {In the past decade, Deep Learning (DL) systems have been widely deployed in various application domains to facilitate our daily life, e.g., natural language processing, healthcare, activity recognition, and autonomous driving. Meanwhile, it is extremely challenging to ensure the correctness of DL systems (e.g., due to their intrinsic nondeterminism), and bugs in DL systems can cause serious consequences and may even threaten human lives. In the literature, researchers have explored various techniques to test, analyze, and verify DL models, since their quality directly affects the corresponding system behaviors. Recently, researchers have also proposed novel techniques for testing the underlying operator-level DL libraries (such as TensorFlow and PyTorch), which provide general binary implementations for each high-level DL operator and are the foundation for running DL models on different hardware platforms. However, there is still limited work targeting the reliability of the emerging tensor compilers (also known as DL compilers), which aim to automatically compile high-level tensor computation graphs directly into high-performance binaries for better efficiency, portability, and scalability than traditional operator-level libraries. Therefore, in this paper, we target the important problem of tensor compiler testing, and have proposed Tzer, a practical fuzzing technique for the widely used TVM tensor compiler. Tzer focuses on mutating the low-level Intermediate Representation (IR) for TVM due to the limited mutation space for the high-level IR. More specifically, Tzer leverages both general-purpose and tensor-compiler-specific mutators guided by coverage feedback for diverse and evolutionary IR mutation; furthermore, since tensor compilers provide various passes (i.e., transformations) for IR optimization, Tzer also performs pass mutation in tandem with IR mutation for more effective fuzzing. Our experimental results show that Tzer substantially outperforms existing fuzzing techniques on tensor compiler testing, with 75\% higher coverage and 50\% more valuable tests than the 2nd-best technique. Also, different components of Tzer have been validated via ablation study. To date, Tzer has detected 49 previously unknown bugs for TVM, with 37 bugs confirmed and 25 bugs fixed (PR merged).},
journal = {Proc. ACM Program. Lang.},
month = {apr},
articleno = {73},
numpages = {26},
keywords = {Compiler Testing, Machine Learning Systems, Fuzzing},
addendum={\textbf{OOPSLA'22}}
}


article{character-inpainting,
	abstract = {Character inpainting is an attractive and challenging task, especially for Chinese calligraphy characters with complex structures and styles. The diversity of Chinese calligraphy styles has created its unique artistic beauty, but specific style features will lead to obvious differences in style and stroke details while recovering. Most current methods are restricted to recover specific characters already present in the training set and retrain the model when recovering characters of new styles. Moreover, these methods are based on edge recovery, which requires the location of the masked area. In this paper, we propose a novel structure-guided generative framework guided by prototype character, which can not only adapt to multiple style fonts but also overall recover glyph structure and strokes without masked information by inferring the style representation. In this case, our method can recover new style characters, which is the first attempt for character inpainting without parameter retraining. Experimental results demonstrate that our method has generalizability and superiority in most application scenarios, compared with several state-of-the-art character inpainting methods.},
	author = {Li, Haolong and Zhong, Zizheng and Guan, Wei and Du, Chenghao and Yang, Yu and Wei, Yuxiang and Ye, Chen},
	da = {2021/09/01},
	date-added = {2023-10-29 20:40:01 -0500},
	date-modified = {2023-10-29 20:40:01 -0500},
	doi = {10.1007/s00371-021-02218-y},
	id = {Li2021},
	isbn = {1432-2315},
	journal = {The Visual Computer},
	number = {9},
	pages = {2895--2906},
	title = {Generative character inpainting guided by structural information},
	ty = {JOUR},
	url = {https://doi.org/10.1007/s00371-021-02218-y},
	volume = {37},
	year = {2021},
	Bdsk-Url-1 = {https://doi.org/10.1007/s00371-021-02218-y}
}
